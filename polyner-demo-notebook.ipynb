{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PolyNER Demo Notebook\n",
    "\n",
    "This notebook demonstrates the key features of the PolyNER library for multilingual Named Entity Recognition with emoji support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's make sure that PolyNER is installed and we have all necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/Users/Yuuv/OneDrive/Desktop/Personal%20Projects/PolyNER\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pandas in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from polyner==0.1.0) (2.2.2)\n",
      "Collecting langdetect (from polyner==0.1.0)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 981.5/981.5 kB 11.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting emoji (from polyner==0.1.0)\n",
      "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: spacy in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from polyner==0.1.0) (3.8.4)\n",
      "Requirement already satisfied: nltk in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from polyner==0.1.0) (3.9.1)\n",
      "Requirement already satisfied: six in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from langdetect->polyner==0.1.0) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from nltk->polyner==0.1.0) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from nltk->polyner==0.1.0) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from nltk->polyner==0.1.0) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from nltk->polyner==0.1.0) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from pandas->polyner==0.1.0) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from pandas->polyner==0.1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from pandas->polyner==0.1.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from pandas->polyner==0.1.0) (2023.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from spacy->polyner==0.1.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from spacy->polyner==0.1.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from spacy->polyner==0.1.0) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from spacy->polyner==0.1.0) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from spacy->polyner==0.1.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from spacy->polyner==0.1.0) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from spacy->polyner==0.1.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from spacy->polyner==0.1.0) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from spacy->polyner==0.1.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from spacy->polyner==0.1.0) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from spacy->polyner==0.1.0) (0.15.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from spacy->polyner==0.1.0) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from spacy->polyner==0.1.0) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from spacy->polyner==0.1.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from spacy->polyner==0.1.0) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from spacy->polyner==0.1.0) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from spacy->polyner==0.1.0) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy->polyner==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->polyner==0.1.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->polyner==0.1.0) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->polyner==0.1.0) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->polyner==0.1.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->polyner==0.1.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->polyner==0.1.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->polyner==0.1.0) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy->polyner==0.1.0) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy->polyner==0.1.0) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from tqdm->nltk->polyner==0.1.0) (0.4.6)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy->polyner==0.1.0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy->polyner==0.1.0) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy->polyner==0.1.0) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy->polyner==0.1.0) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from jinja2->spacy->polyner==0.1.0) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->polyner==0.1.0) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->polyner==0.1.0) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->polyner==0.1.0) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->polyner==0.1.0) (0.1.0)\n",
      "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
      "   ---------------------------------------- 0.0/590.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 590.6/590.6 kB 10.6 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993251 sha256=18246bd26c38b47087c02115c89c041624260c6c06d1ff2dbc3c6353075b1c36\n",
      "  Stored in directory: c:\\users\\yuuv\\appdata\\local\\pip\\cache\\wheels\\c1\\67\\88\\e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect, emoji, polyner\n",
      "  Running setup.py develop for polyner\n",
      "Successfully installed emoji-2.14.1 langdetect-1.0.9 polyner-0.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Legacy editable install of polyner==0.1.0 from file:///C:/Users/Yuuv/OneDrive/Desktop/Personal%20Projects/PolyNER (setup.py develop) is deprecated. pip 25.0 will enforce this behaviour change. A possible replacement is to add a pyproject.toml or enable --use-pep517, and use setuptools >= 64. If the resulting installation is not behaving as expected, try using --config-settings editable_mode=compat. Please consult the setuptools documentation for more information. Discussion can be found at https://github.com/pypa/pip/issues/11457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yuuv\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to install polyner if not already installed\n",
    "# !pip install polyner\n",
    "\n",
    "# For development installation (if working from a local clone of the repo)\n",
    "!pip install -e .\n",
    "\n",
    "# We'll also need these packages for visualization\n",
    "!pip install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Yuuv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# Import PolyNER and related libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up matplotlib for better visualization\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# Import PolyNER and related utilities\n",
    "from polyner import PolyNER\n",
    "from polyner.utils import (\n",
    "    filter_by_language, \n",
    "    filter_emojis, \n",
    "    filter_by_entity,\n",
    "    get_language_distribution, \n",
    "    get_entity_distribution,\n",
    "    get_emoji_distribution\n",
    ")\n",
    "from polyner.entity_recognition import DictionaryEntityRecognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 1: Basic Text Processing\n",
    "\n",
    "Let's start with processing a simple English text to see the basic output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Yuuv/nltk_data'\n    - 'c:\\\\Users\\\\Yuuv\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\Yuuv\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Yuuv\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Yuuv\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Process a simple English text\u001b[39;00m\n\u001b[0;32m      5\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApple Inc. is headquartered in Cupertino, California. The company was founded by Steve Jobs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m result \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mprocess(text)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Display the result\u001b[39;00m\n\u001b[0;32m      9\u001b[0m result\n",
      "File \u001b[1;32mc:\\Users\\Yuuv\\OneDrive\\Desktop\\Personal Projects\\PolyNER\\polyner\\core.py:62\u001b[0m, in \u001b[0;36mPolyNER.process\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     59\u001b[0m emojis \u001b[38;5;241m=\u001b[39m extract_emojis(text)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Tokenize the text\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenize_text(text)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Process each token\u001b[39;00m\n\u001b[0;32m     65\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Yuuv\\OneDrive\\Desktop\\Personal Projects\\PolyNER\\polyner\\tokenization.py:58\u001b[0m, in \u001b[0;36mtokenize_text\u001b[1;34m(text, preserve_emojis)\u001b[0m\n\u001b[0;32m     55\u001b[0m             tokens[i] \u001b[38;5;241m=\u001b[39m placeholders[token]\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# If we don't need to preserve emojis, just tokenize normally\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Filter out empty tokens\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;129;01mor\u001b[39;00m is_emoji(token)]\n",
      "File \u001b[1;32mc:\\Users\\Yuuv\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Yuuv\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\Yuuv\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32mc:\\Users\\Yuuv\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32mc:\\Users\\Yuuv\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\Yuuv\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Yuuv/nltk_data'\n    - 'c:\\\\Users\\\\Yuuv\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\Yuuv\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Yuuv\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Yuuv\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Initialize the PolyNER processor\n",
    "processor = PolyNER()\n",
    "\n",
    "# Process a simple English text\n",
    "text = \"Apple Inc. is headquartered in Cupertino, California. The company was founded by Steve Jobs.\"\n",
    "result = processor.process(text)\n",
    "\n",
    "# Display the result\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Output\n",
    "\n",
    "Let's examine the columns in the output DataFrame:\n",
    "\n",
    "- `token`: The individual word or character\n",
    "- `language`: Detected language code (e.g., 'en' for English)\n",
    "- `is_emoji`: Boolean flag indicating if the token is an emoji\n",
    "- `norm_token`: Normalized version of the token (lowercase, accents removed)\n",
    "- `entity_label`: Named entity type if recognized (e.g., 'ORG' for organization, 'PERSON' for person)\n",
    "\n",
    "Now, let's look at just the entities that were detected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to show only detected entities\n",
    "entities = filter_by_entity(result)\n",
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 2: Multilingual Support\n",
    "\n",
    "Let's test PolyNER's ability to handle multiple languages in the same text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text with multiple languages\n",
    "multilingual_text = \"\"\"\n",
    "Hello, my name is John Smith. I work for Microsoft in Seattle.\n",
    "Bonjour, je m'appelle Pierre Dupont. Je travaille pour Google à Paris.\n",
    "Hola, me llamo Carlos García. Trabajo para Amazon en Madrid.\n",
    "你好，我叫李明。我在北京的百度工作。\n",
    "\"\"\"\n",
    "\n",
    "# Process the multilingual text\n",
    "multi_result = processor.process(multilingual_text)\n",
    "\n",
    "# Get language distribution\n",
    "lang_dist = get_language_distribution(multi_result)\n",
    "\n",
    "# Display the language distribution\n",
    "print(\"Language distribution:\")\n",
    "for lang, count in lang_dist.items():\n",
    "    print(f\"{lang}: {count} tokens\")\n",
    "\n",
    "# Display a sample of each detected language\n",
    "for lang in lang_dist.keys():\n",
    "    print(f\"\\nSample of tokens in {lang}:\")\n",
    "    lang_tokens = filter_by_language(multi_result, lang)\n",
    "    print(lang_tokens['token'].head().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize language distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(lang_dist.keys(), lang_dist.values())\n",
    "plt.title('Token Distribution by Language')\n",
    "plt.xlabel('Language')\n",
    "plt.ylabel('Number of Tokens')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 3: Entity Recognition Across Languages\n",
    "\n",
    "Let's see how well PolyNER recognizes entities in different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display entities from the multilingual text\n",
    "multi_entities = filter_by_entity(multi_result)\n",
    "multi_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get entity distribution by type\n",
    "entity_dist = get_entity_distribution(multi_result)\n",
    "\n",
    "# Visualize entity distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(entity_dist.keys(), entity_dist.values())\n",
    "plt.title('Entity Distribution by Type')\n",
    "plt.xlabel('Entity Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 4: Emoji Handling\n",
    "\n",
    "Let's test how PolyNER handles text with emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text with emojis\n",
    "emoji_text = \"\"\"\n",
    "I love visiting New York City! 🗽 The Statue of Liberty is amazing. 😍\n",
    "Tokyo 🗼 is also on my list, and I want to see the cherry blossoms! 🌸\n",
    "Paris and the Eiffel Tower 🇫🇷 are romantic, especially at night. ✨\n",
    "I'm saving up to go on a world tour! 🌍 ✈️ 🧳\n",
    "\"\"\"\n",
    "\n",
    "# Process the text\n",
    "emoji_result = processor.process(emoji_text)\n",
    "\n",
    "# Extract emojis\n",
    "emojis = filter_emojis(emoji_result)\n",
    "emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get emoji distribution\n",
    "emoji_dist = get_emoji_distribution(emoji_result)\n",
    "\n",
    "# Display emoji distribution\n",
    "print(\"Emoji distribution:\")\n",
    "for emoji, count in emoji_dist.items():\n",
    "    print(f\"{emoji}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 5: Custom Entity Dictionaries\n",
    "\n",
    "One of PolyNER's powerful features is the ability to add custom entity dictionaries for domain-specific recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dictionary recognizer\n",
    "dict_recognizer = DictionaryEntityRecognizer()\n",
    "\n",
    "# Add custom dictionaries for tech products and companies\n",
    "dict_recognizer.add_entity_dictionary(\n",
    "    \"SMARTPHONE\",\n",
    "    [\"iPhone 13\", \"iPhone 12\", \"Galaxy S21\", \"Pixel 6\", \"OnePlus 9\"],\n",
    "    case_sensitive=True\n",
    ")\n",
    "\n",
    "dict_recognizer.add_entity_dictionary(\n",
    "    \"LAPTOP\",\n",
    "    [\"MacBook Pro\", \"MacBook Air\", \"Dell XPS\", \"ThinkPad X1\", \"Surface Laptop\"],\n",
    "    case_sensitive=True\n",
    ")\n",
    "\n",
    "dict_recognizer.add_entity_dictionary(\n",
    "    \"TECH_COMPANY\",\n",
    "    [\"Apple\", \"Google\", \"Microsoft\", \"Amazon\", \"Meta\", \"Samsung\", \"Tesla\"],\n",
    "    case_sensitive=True\n",
    ")\n",
    "\n",
    "# Sample text about tech products\n",
    "tech_text = \"\"\"\n",
    "I recently upgraded from my old iPhone 12 to the new iPhone 13 Pro Max, and the camera is amazing!\n",
    "My friend uses a Galaxy S21 and loves it too. For work, I use a MacBook Pro, but I'm considering\n",
    "switching to a Dell XPS or ThinkPad X1 for better compatibility with our office software.\n",
    "\n",
    "Apple and Google are releasing new AI features, while Microsoft is focusing on cloud services.\n",
    "Meta (formerly Facebook) is investing heavily in VR technology.\n",
    "\"\"\"\n",
    "\n",
    "# Recognize entities using the custom dictionaries\n",
    "custom_entities = dict_recognizer.recognize_entities(tech_text)\n",
    "\n",
    "# Display the results\n",
    "print(\"Detected tech entities:\")\n",
    "for entity in custom_entities:\n",
    "    print(f\"{entity['text']} - {entity['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count entities by type\n",
    "entity_counts = {}\n",
    "for entity in custom_entities:\n",
    "    label = entity[\"label\"]\n",
    "    if label not in entity_counts:\n",
    "        entity_counts[label] = 0\n",
    "    entity_counts[label] += 1\n",
    "\n",
    "# Display entity counts\n",
    "print(\"\\nEntity counts by type:\")\n",
    "for label, count in entity_counts.items():\n",
    "    print(f\"{label}: {count}\")\n",
    "\n",
    "# Visualize entity counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(entity_counts.keys(), entity_counts.values())\n",
    "plt.title('Custom Entity Distribution by Type')\n",
    "plt.xlabel('Entity Type')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 6: Combined Approach\n",
    "\n",
    "Let's demonstrate how to combine the standard NER with custom dictionaries for a comprehensive approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dictionary recognizer for medical terms\n",
    "medical_recognizer = DictionaryEntityRecognizer()\n",
    "\n",
    "# Add medical dictionaries\n",
    "medical_recognizer.add_entity_dictionary(\n",
    "    \"MEDICATION\",\n",
    "    [\"Aspirin\", \"Ibuprofen\", \"Paracetamol\", \"Amoxicillin\", \"Lipitor\", \"Prozac\", \"Metformin\"],\n",
    "    case_sensitive=True\n",
    ")\n",
    "\n",
    "medical_recognizer.add_entity_dictionary(\n",
    "    \"CONDITION\",\n",
    "    [\"Hypertension\", \"Diabetes\", \"Type 2 Diabetes\", \"Asthma\", \"Arthritis\", \"Depression\"],\n",
    "    case_sensitive=True\n",
    ")\n",
    "\n",
    "# Sample medical text\n",
    "medical_text = \"\"\"\n",
    "Patient John Smith (42) was admitted to Massachusetts General Hospital in Boston on June 15, 2023.\n",
    "He has a history of Hypertension and Type 2 Diabetes. Current medications include Lipitor (20mg daily)\n",
    "and Metformin (500mg twice daily). The patient reported taking Ibuprofen for occasional headaches.\n",
    "Dr. Sarah Johnson recommended continuing current treatment and scheduled a follow-up appointment\n",
    "in 3 months at the clinic in New York.\n",
    "\"\"\"\n",
    "\n",
    "# Process with standard NER\n",
    "standard_result = processor.process(medical_text)\n",
    "\n",
    "# Display standard NER results\n",
    "print(\"Standard NER results:\")\n",
    "std_entities = standard_result[standard_result[\"entity_label\"].notna()]\n",
    "for _, row in std_entities.iterrows():\n",
    "    print(f\"{row['token']} - {row['entity_label']}\")\n",
    "\n",
    "# Process with custom medical dictionary\n",
    "medical_entities = medical_recognizer.recognize_entities(medical_text)\n",
    "\n",
    "# Display custom medical entities\n",
    "print(\"\\nCustom medical entities:\")\n",
    "for entity in medical_entities:\n",
    "    print(f\"{entity['text']} - {entity['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 7: Batch Processing\n",
    "\n",
    "Let's demonstrate how to process multiple texts at once using the batch processing feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of texts\n",
    "texts = [\n",
    "    \"Apple Inc. is headquartered in Cupertino, California.\",\n",
    "    \"Google's main office is in Mountain View.\",\n",
    "    \"Amazon was founded by Jeff Bezos in Seattle.\",\n",
    "    \"Microsoft is based in Redmond, Washington.\"\n",
    "]\n",
    "\n",
    "# Process the batch\n",
    "batch_results = processor.process_batch(texts)\n",
    "\n",
    "# Display entities from each text\n",
    "for i, result in enumerate(batch_results):\n",
    "    print(f\"Text {i+1}: {texts[i]}\")\n",
    "    entities = result[result[\"entity_label\"].notna()]\n",
    "    if not entities.empty:\n",
    "        print(\"Detected entities:\")\n",
    "        for _, row in entities.iterrows():\n",
    "            print(f\"  {row['token']} - {row['entity_label']}\")\n",
    "    else:\n",
    "        print(\"No entities detected.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 8: Analysis of Social Media Content\n",
    "\n",
    "Social media content often contains a mix of languages, emojis, and entities. Let's see how PolyNER handles this type of content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample social media post\n",
    "social_media_post = \"\"\"\n",
    "Just had an amazing dinner at Le Petit Bistro in Paris! 🍷 The escargot was délicieux! \n",
    "Next stop: Berlin for Oktoberfest! 🍺 Can't wait to try some authentic German bratwurst.\n",
    "私は日本食も大好きです。寿司と天ぷらは最高です！🍣\n",
    "Follow my food adventures on Instagram @foodie_traveler and check out my latest YouTube video!\n",
    "\"\"\"\n",
    "\n",
    "# Process the social media post\n",
    "social_result = processor.process(social_media_post)\n",
    "\n",
    "# Display language distribution\n",
    "social_lang_dist = get_language_distribution(social_result)\n",
    "print(\"Language distribution:\")\n",
    "for lang, count in social_lang_dist.items():\n",
    "    print(f\"{lang}: {count} tokens\")\n",
    "\n",
    "# Display emojis\n",
    "social_emojis = filter_emojis(social_result)\n",
    "print(\"\\nEmojis:\")\n",
    "print(social_emojis['token'].tolist())\n",
    "\n",
    "# Display entities\n",
    "social_entities = filter_by_entity(social_result)\n",
    "print(\"\\nDetected entities:\")\n",
    "for _, row in social_entities.iterrows():\n",
    "    print(f\"{row['token']} - {row['entity_label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Analysis: Visualizing the Results\n",
    "\n",
    "Let's create a more comprehensive visualization of the analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_and_visualize(text, title=\"Text Analysis\"):\n",
    "    # Process the text\n",
    "    result = processor.process(text)\n",
    "    \n",
    "    # Get distributions\n",
    "    lang_dist = get_language_distribution(result)\n",
    "    entity_dist = get_entity_distribution(result)\n",
    "    emoji_dist = get_emoji_distribution(result)\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    \n",
    "    # Plot language distribution\n",
    "    if lang_dist:\n",
    "        axs[0].bar(lang_dist.keys(), lang_dist.values())\n",
    "        axs[0].set_title('Language Distribution')\n",
    "        axs[0].set_xlabel('Language')\n",
    "        axs[0].set_ylabel('Token Count')\n",
    "        axs[0].tick_params(axis='x', rotation=45)\n",
    "    else:\n",
    "        axs[0].text(0.5, 0.5, 'No language data', ha='center', va='center')\n",
    "        axs[0].set_title('Language Distribution')\n",
    "    \n",
    "    # Plot entity distribution\n",
    "    if entity_dist:\n",
    "        axs[1].bar(entity_dist.keys(), entity_dist.values(), color='green')\n",
    "        axs[1].set_title('Entity Distribution')\n",
    "        axs[1].set_xlabel('Entity Type')\n",
    "        axs[1].set_ylabel('Count')\n",
    "        axs[1].tick_params(axis='x', rotation=45)\n",
    "    else:\n",
    "        axs[1].text(0.5, 0.5, 'No entities detected', ha='center', va='center')\n",
    "        axs[1].set_title('Entity Distribution')\n",
    "    \n",
    "    # Plot emoji distribution\n",
    "    if emoji_dist:\n",
    "        emojis = list(emoji_dist.keys())\n",
    "        counts = list(emoji_dist.values())\n",
    "        axs[2].bar(range(len(emojis)), counts, color='orange')\n",
    "        axs[2].set_title('Emoji Distribution')\n",
    "        axs[2].set_xlabel('Emoji')\n",
    "        axs[2].set_ylabel('Count')\n",
    "        axs[2].set_xticks(range(len(emojis)))\n",
    "        axs[2].set_xticklabels(emojis, fontsize=14)\n",
    "    else:\n",
    "        axs[2].text(0.5, 0.5, 'No emojis detected', ha='center', va='center')\n",
    "        axs[2].set_title('Emoji Distribution')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "    \n",
    "    # Return the processed result for further analysis\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and visualize the social media post\n",
    "social_result = analyze_and_visualize(social_media_post, \"Social Media Post Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and visualize the news article\n",
    "news_article = \"\"\"\n",
    "WASHINGTON (Reuters) - U.S. President Joe Biden met with Chinese President Xi Jinping \n",
    "at the G20 summit in Rome on Tuesday. The leaders discussed trade relations between \n",
    "the United States and China, as well as climate change initiatives ahead of the \n",
    "COP26 conference in Glasgow, Scotland.\n",
    "\n",
    "Meanwhile, tech giants Apple and Google announced new AI partnerships, \n",
    "while Tesla's stock surged 5% after reporting record quarterly earnings.\n",
    "\"\"\"\n",
    "\n",
    "news_result = analyze_and_visualize(news_article, \"News Article Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated the key features of the PolyNER library:\n",
    "\n",
    "1. Basic text processing and entity recognition\n",
    "2. Multilingual support\n",
    "3. Emoji handling\n",
    "4. Custom entity dictionaries\n",
    "5. Combined approaches for domain-specific entity recognition\n",
    "6. Batch processing\n",
    "7. Analysis of mixed content (like social media posts)\n",
    "8. Visualization of analysis results\n",
    "\n",
    "PolyNER provides a comprehensive toolkit for processing and analyzing text that contains multiple languages, emojis, and specialized domain terminology."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
